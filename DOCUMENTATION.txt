================================================================================
                    WINNIE DA POOH - DOCUMENTATION
================================================================================

Winnie Da Pooh is a reproducible forecasting data pipeline and benchmarking 
framework. It normalizes data from prediction markets (Kalshi) and forecasting 
platforms (Metaculus) into a unified format for testing forecasting models.

--------------------------------------------------------------------------------
1. QUICK START / USAGE
--------------------------------------------------------------------------------

Prerequisites: Python 3.13+, `uv` (package manager).

| Step | Command | Description |
|------|---------|-------------|
| 1 | `uv sync` | Install dependencies. |
| 2 | `cp .env.example .env` | Configure API keys (Kalshi/Metaculus). |
| 3 | `uv run scripts/build_db.py` | Fetch data and build the unified Parquet dataset. |
| 4 | `uv run runner/runner.py --method mlp_nn` | Run a benchmarking experiment. |
| 5 | `uv run scripts/inspect_parquet.py` | View summary statistics of the latest dataset. |

--------------------------------------------------------------------------------
2. DATA SCHEMA
--------------------------------------------------------------------------------

The pipeline normalizes data into two primary structures, stored in a unified 
Parquet format.

A. MarketRecord (The "Static" Info)
Defined in `src/common/schema.py`.

| Field | Type | Description |
|-------|------|-------------|
| source | str | "kalshi" or "metaculus" |
| market_id | str | Unique ID within the source. |
| title | str | Question/Market name. |
| description | str | Full context of the question. |
| url | str | Link to the original platform. |
| market_type | Enum | binary, multiple_choice, numeric, or other. |
| answer_options_json| str (JSON) | List of possible answers/choices. |
| end_time | datetime | When the market closes. |
| status | Enum | open, closed, resolved, or unknown. |
| resolved_value_json| str (JSON) | The outcome (e.g., "1.0", "yes", or list). |
| metadata_json | str (JSON) | Source-specific raw metadata. |

B. TimeSeriesPoint (The "Belief" History)
Stored in the same Parquet table as nested/exploded rows for history.

| Field | Type | Description |
|-------|------|-------------|
| ts | datetime | Timestamp of the prediction/trade. |
| belief_scalar | float | The normalized probability or price (0-1). |
| belief_json | str (JSON) | Non-scalar beliefs (e.g., distributions). |
| volume | float | Trading volume (Kalshi-specific). |
| open_interest | float | Active contracts (Kalshi-specific). |
| bid / ask | float | Best buy/sell prices (Kalshi-specific). |

--------------------------------------------------------------------------------
3. CORE CLASSES & MODULES
--------------------------------------------------------------------------------

A. Data Handling (`dataobject/`)
- `MarketDataset`: Entry point for loading `.parquet` data.
- `MarketRecordWrapper`: Helper to access row data with validation.
- `SplitManager`: Handles reproducible Train/Test/Bench splits via hashing.

B. Task Definitions (`dataobject/tasks/`)
- `Task`: Abstract base class for defining prediction problems.
- `ResolveBinary`: Standard binary classification task (predict the final result).
- `Example`: A single data point (features + history + target).
- `Batch`: A collection of `Example` objects.

C. Forecasting Methods (`methods/`)
- `ForecastMethod`: Base class for all models/baselines. Supports `.fit()`, `.predict()`, `.save()`, `.load()`.
- `LastPriceBaseline`: Simple baseline that predicts the most recent belief.
- `MLPForecaster`: A simple Neural Network (MLP) trained on price history.
- `registry.py`: Central factory for instantiating methods.

D. Experiment Workflow (`runner/`)
- `RunSpec`: Serializable configuration for an experiment run.
- `runner.py`: The primary CLI for orchestrating data loading, training, and evaluation.
- `evaluator.py`: Computes metrics (Brier, LogLoss, etc.) per task.

--------------------------------------------------------------------------------
4. HOW TO BUILD OFF THIS REPO (Best Practices)
--------------------------------------------------------------------------------

A. Adding a New Forecasting Method
1. **Create**: New file in `methods/` (e.g., `methods/my_model.py`).
2. **Subclass**: Inherit from `ForecastMethod`.
3. **Implement**:
   - `.fit(train_batches, spec)`: Logic to train your model.
   - `.predict(batch, spec)`: Return a list of predictions (0-1).
   - `.save(path)` / `.load(path)`: (Optional) Handle persistence.
4. **Register**: Add your class to `METHODS` in `methods/registry.py`.

B. Adding a New Task
1. **Create**: New file in `dataobject/tasks/` (e.g., `dataobject/tasks/my_task.py`).
2. **Subclass**: Inherit from `Task`.
3. **Implement**:
   - `.make_examples(record, rng)`: Define how to turn a `MarketRecord` into `Example` objects.
   - `.metric_fns()`: Return a dict of `{name: sklearn_metric_fn}`.
4. **Register**: Add it to `TASK_REGISTRY` in `runner/runner.py`.

C. Experiment Artifacts
All outputs are stored in `data/outputs/` (ignored by git), organized by model configuration and run:
`data/outputs/<model>_<hash>/run_<timestamp>_<name>/`

Sub-directories within each run:
- `model/`: Serialized model weights (`.joblib`, `.pt`).
- `logs/`: Execution logs and `run.log`.
- `plots/`: Visualizations and performance graphs.
- `metrics/`: Structured JSON metrics (`metrics.json`) and run spec (`spec.json`).
- `*.txt`: A run-info file with description and metadata.

--------------------------------------------------------------------------------
5. REPOSITORY STRUCTURE
--------------------------------------------------------------------------------

/
├── data/               # Local data storage (raw, clean parquets, outputs)
│   └── outputs/        # Organized experiment artifacts (models, logs, plots, metrics)
├── dataobject/         # Data models, tasks, and IO hygiene
├── docs/               # Platform-specific documentation/metadata
├── methods/            # Implementations of forecasting algorithms
├── runner/             # Experiment orchestration and evaluation
├── scripts/            # Data ingestion and inspection tools
├── src/                # Core logic (normalization, API grabbers)
│   ├── common/         # Shared utils (schema, config, http)
│   ├── kalshi/         # Kalshi-specific mapping
│   └── metaculus/      # Metaculus-specific mapping
├── pyproject.toml      # Project dependencies and metadata
└── README.md           # High-level overview
